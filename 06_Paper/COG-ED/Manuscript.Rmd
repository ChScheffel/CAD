---
title             : "COG-ED Manuscript"
author            : "Josephine"
date              : "13 8 2021"
output            : html_document
bibliography      : ref_COG-ED.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bibtex)
```

# Introduction

In everyday life, effort and reward are closely intertwined [@Botvinick2009].
With each decision a person makes, they have to evaluate whether the effort required to reach a goal is worth being exerted, given the reward they receive when reaching the goal.
A reward is subjectively more valuable if it is obtained with less effort, so the required effort is used as a reference point for estimating the reward value [@Botvinick2009].
However, the cost of the effort itself is also subjective, and research has not yet established which function best describes the relationship between effort and cost [@Kool2018].
Investigating effort and cost is challenging because “effort is not a property of the target task alone, but also a function of the individual’s cognitive capacities, as well as the degree of effort voluntarily mobilized for the task, which in turn is a function of the individual’s reward sensitivity” [@Kool2018, p. 209].

One task that is often used to investigate effort is the n-back task, a working memory task in which a continuous stream of stimuli, e.g. letters, is presented on screen and participants indicate via button press whether the current stimulus is the same as *n* stimuli before, with *n* being the level of difficulty between one and six [@Mackworth1959].
It requires simultaneous storage of past stimuli and processing of new stimuli [@Jonides1997], and participants are asked to respond as quickly and as accurately as they can.
The n-back task is well suited to investigate effort because it is an almost continuous manipulation of task load, as has been shown by monotonic increases in error rates and reaction times [@Jaeggi2010] as well as monotonic increases in brain activity in areas associated with working memory [@Jonides1997; @Owen2005].
However, reliability measures of the n-back task are mixed, presumably due to ceiling effects in performance in the easiest level, causing inconsistent associations of n-back performance and measures such as executive functioning and fluid intelligence [@Jaeggi2010].

A way to quantify the subjective cost of each n-back level has been developed by Westbrook et al. [-@Westbrook2013], called the Cognitive Effort Discounting Paradigm (COG-ED).
First, the participants complete n-back level one to six to familiarize themselves with the task.
Then, each level two to six is being compared with level one by asking the participants to decide between receiving 2\$ for the more difficult level or 1\$ for level one.
If they choose the more difficult level, the reward for level one increases by 0.50\$, if the choose level one, it decreases by 0.50\$.
This is repeated five more times, with each adjustment of the level one reward being half as big as in the previous step, while the reward for the more difficult level remains fixed at 2\$.
The idea is to estimate the participants' subjective cost of the more difficult level by identifying the smallest reward they would still be okay with being paid for doing level one.
Therefore, the higher the difficult level, the smaller the reward that the participants would accept for level one.
The subjective value (SV) of each difficult level is then calculated by dividing the final reward value of level one by the fixed 2\$ reward of the more difficult level.
Westbrook et al. [-@Westbrook2013] used these SVs to investigate interindividual differences in effort discounting by plotting the SVs and computing an area under the curve (AUC).
Younger participants had a larger AUC than older participants, i.e. they needed a lower monetary incentive for choosing the more difficult levels over level one.

The AUC in the study by Westbrook et al. [-@Westbrook2013] was also associated with the participants' Need for Cognition (NFC) score, a personality trait describing individuals who actively seek and enjoy effortful cognitive activities [@Cacioppo1982].
On the surface, this association stands to reason, as individuals with higher NFC are more motivated to mobilize cognitive effort because they perceive it as intrinsically rewarding.
However, the relation of NFC and SVs might be confounded with task performance itself, since subjective task values have been shown to depend on both the exerted effort [@Wang2017] and the expected performance [@Yee2021].
Findings on NFC and n-back performance are heterogenous.
On one hand individuals with higher NFC neither had better n-back performance [@Gaertner2021] nor better working memory [@Fleischhauer 2010; @Hill2013], on the other hand they showed greater attention allocation [@Enge2008] and brain flexibility [@He2019].
Nevertheless, a possible association of NFC and task performance might be secondary, since task load has been shown to be a better predictor of SVs than task performance [@Culbreth2016; @Westbrook2013; @Westbrook2019].
But since other studies utilizing the COG-ED paradigm found the association of NFC and SVs to disappear after correcting for performance [@Kramer2021] or found no association of NFC and SVs at all [@Crawford2021], more research is needed to shed light on this issue.

The present study contributes to the investigation of effort and SVs by changing one fundamental assumption of the original COG-ED paradigm: The assumption that the easiest n-back level has the highest SV.
We adapted the COG-ED paradigm in such a way that it allows the computation of SVs for different n-back levels without presuming that all individuals inherently prefer 1-back.
In fact, our COG-ED adaptation allows the computations of SVs, even if the individual does not have a clear preference for one task level.
In the present study, we will validate this adaptation by replicating the findings of Westbrook et al. [-@Westbrook2013].
Additionally, we will apply the adaption to a task with different emotion regulation strategies to establish an effort discounting paradigm for task varieties that have no objective order of task load.
The COG-ED paradigm has been applied to tasks with different domains before, showing that SVs across task domains correlate [@Crawford2021], but it has not been applied to tasks without objective task load so far.

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}

  library(here)         # to set a directory without defined paths
  library(tidyverse)    # plotting and managing data
  

```

```{r import, echo=FALSE, message=FALSE, warning=FALSE}

  # set top level directory to source file

  here::i_am("Manuscript.Rmd")
  
  # import n-back and effort discounting data into a list each

  datalist_nback = lapply(list.files(here("Sampledata"), pattern = '.*nback.*csv', full.names = TRUE),
                          read.csv, stringsAsFactors = FALSE, header = TRUE)
  datalist_ED = lapply(list.files(here("Sampledata"), pattern = '.*ED.*csv', full.names = TRUE),
                          read.csv, stringsAsFactors = FALSE, header = TRUE)
  
  # create empty data frames for loops to feed into
  
  data_nback <- data.frame(subject = character(), level = double(), target = character(),
                           response = character(), correct = double(), rt = double())
  data_ED <- data.frame(subject = character(), step = double(), choice = character(),
                           LBvalue = double(), LBlevel = double(), RBvalue = double(), RBlevel = double())

  
  # put relevant data into a dataframes
  
  for (i in 1:length(datalist_nback)) {
    
    newdata <- data.frame(datalist_nback[[i]][["Participant"]], datalist_nback[[i]][["currentlevel"]],
                          datalist_nback[[i]][["Stimulus"]], datalist_nback[[i]][["trial_resp.keys"]],
                          datalist_nback[[i]][["trial_resp.corr"]], datalist_nback[[i]][["trial_resp.rt"]])
    colnames(newdata) <- names(data_nback)
    data_nback <- rbind(data_nback, newdata)
  }
  
  for (i in 1:length(datalist_ED)) {
    
    newdata <- data.frame(datalist_ED[[i]][["Participant"]][2:length(datalist_ED[[i]][["Participant"]])],
                          datalist_ED[[i]][["EDround.thisN"]][2:length(datalist_ED[[i]][["EDround.thisN"]])],
                          datalist_ED[[i]][["EDclick.clicked_name"]][2:length(datalist_ED[[i]][["EDclick.clicked_name"]])],
                          datalist_ED[[i]][["EDleftbutton.value"]][2:length(datalist_ED[[i]][["EDleftbutton.value"]])],
                          datalist_ED[[i]][["EDleftbutton.nback"]][2:length(datalist_ED[[i]][["EDleftbutton.nback"]])],
                          datalist_ED[[i]][["EDrightbutton.value"]][2:length(datalist_ED[[i]][["EDrightbutton.value"]])],
                          datalist_ED[[i]][["EDrightbutton.nback"]][2:length(datalist_ED[[i]][["EDrightbutton.nback"]])])
    colnames(newdata) <- names(data_ED)
    data_ED <- rbind(data_ED, newdata)
  }
  
  # replace some columns with 'easy to work with' values
  
  # the target column will contain 1 for targets and 0 for all non-targets
  data_nback[data_nback == "Target"] <- 1
  data_nback[data_nback == "Pretarget"|data_nback == "Lure"|data_nback == "Distractor"] <- 0
  data_nback$target <- as.numeric(data_nback$target)
  # the response column will contain 1 for responses and 0 for all non-responses
  data_nback[data_nback == "left"|data_nback == "right"] <- 1
  data_nback[data_nback == "None"] <- 0
  data_nback$response <- as.numeric(data_nback$response)
  # the choice column will contain 1 for the left button and 2 for the right button
  data_ED[data_ED == "EDleftbutton"] <- 1
  data_ED[data_ED == "EDrightbutton"] <- 2
  data_ED$choice <- as.numeric(data_ED$choice)
  
  # since only the last choice of each comparison is relevant, we will keep only those rows
  
  data_ED <- data_ED[data_ED$step == 6,]
  data_ED <- subset(data_ED, select = -c(step))

```

```{r SVcomputation, echo=FALSE, message=FALSE, warning=FALSE}

  # apply the addition or subtraction of 0.015625 to the last choices

  for (i in 1:nrow(data_ED)) {
    
    data_ED$fixedlevel[i] <- data_ED[i,grep("Bvalue", colnames(data_ED))[which(data_ED[i,grep("Bvalue", colnames(data_ED))] == 2.00)] + 1]
    data_ED$flexlevel[i] <- data_ED[i,grep("Bvalue", colnames(data_ED))[which(data_ED[i,grep("Bvalue", colnames(data_ED))] != 2.00)] + 1]
    
    if (data_ED$choice[i] == 1) {
      if (data_ED$LBvalue[i] == 2.00) {
        data_ED$flexvalue[i] <- round(data_ED$RBvalue[i] + 0.015625, digits = 2)
      } else {
        data_ED$flexvalue[i] <- round(data_ED$LBvalue[i] - 0.015625, digits = 2)
      }
    } else {
      if (data_ED$RBvalue[i] == 2.00) {
        data_ED$flexvalue[i] <- round(data_ED$LBvalue[i] + 0.015625, digits = 2)
      } else {
        data_ED$flexvalue[i] <- round(data_ED$RBvalue[i] - 0.015625, digits = 2)
      }
    }
    
  }

  # create vector indicating in which rows the data of a new subject begins

  subjectindex <- c(1,which(data_ED$subject != dplyr::lag(data_ED$subject)),nrow(data_ED))
  
  # create empty data frame for the loop to feed into
  
  data_SV <- data.frame(subject = character(), sv1 = double(), sv2 = double(), sv3 = double(), sv4 = double())
  
  # compute subjective values per n-back level and feed into data frame
  
  for (i in 1:(length(subjectindex)-1)) {
    
    # initialize empty vectors
    
    tempone <- double()
    temptwo <- double()
    tempthree <- double()
    tempfour <- double()
    
    # check for every number, whether it appears in the fixedlevel and flexlevel columns
    # divide flexvalue by 2 if it appears in the fixedlevel columns
    # append 1 if it appears in the flexlevel column
    
    # for 1-back
    
        if (is_empty(which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 1)) == FALSE) {
          tempone <- append(tempone, data_ED$flexvalue[subjectindex[i]-1 +
                                                         which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 1)]/2)
        } else {
          # do nothing
        }
        
        if (is_empty(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 1)) == FALSE) {
          tempone <- append(tempone, rep(1, length(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 1))))
        } else {
          # do nothing
        }
    
    # for 2-back
    
        if (is_empty(which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 2)) == FALSE) {
          temptwo <- append(temptwo, data_ED$flexvalue[subjectindex[i]-1 +
                                                         which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 2)]/2)
        } else {
          # do nothing
        }
        
        if (is_empty(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 2)) == FALSE) {
          temptwo <- append(temptwo, rep(1, length(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 2))))
        } else {
          # do nothing
        }
    
    # for 3-back
    
        if (is_empty(which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 3)) == FALSE) {
          tempthree <- append(tempthree, data_ED$flexvalue[subjectindex[i]-1 +
                                                             which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 3)]/2)
        } else {
          # do nothing
        }
        
        if (is_empty(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 3)) == FALSE) {
          tempthree <- append(tempthree, rep(1, length(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 3))))
        } else {
          # do nothing
        }
    
    # for 4-back
    
        if (is_empty(which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 4)) == FALSE) {
          tempfour <- append(tempfour, data_ED$flexvalue[subjectindex[i]-1 +
                                                           which(data_ED$fixedlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 4)]/2)
        } else {
          # do nothing
        }
        
        if (is_empty(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 4)) == FALSE) {
          tempfour <- append(tempfour, rep(1, length(which(data_ED$flexlevel[c(subjectindex[i]:(subjectindex[i+1]-1))] == 4))))
        } else {
          # do nothing
        }
    
    # put the mean subjective value of the three values per n-back level in the respective column and add to data frame
    
    newdata <- data.frame(data_ED$subject[subjectindex[i]], mean(tempone), mean(temptwo), mean(tempthree), mean(tempfour))
    colnames(newdata) <- names(data_SV)
    data_SV <- rbind(data_SV, newdata)
    
  }
```

```{r Hypothesis_1a, echo=FALSE, message=FALSE, warning=FALSE}

  # H1a: Performance measured by signal detection d’ declines with increasing n-back level.

  # set the number of targets and non-targets, so in case the paradigm will be changed it is easily accessible
  # they are multiplied by 2, because there are two runs per n-back level

  num_targets <- 16*2
  num_nontargets <- 48*2
  
  # compute index of rows in which the levels change
  
  levelindex <- c(1,which(data_nback$level != dplyr::lag(data_nback$level)),nrow(data_nback))
  
  # set up empty data frame for the loop to feed into
  
  dprime <- data.frame(subject = character(), level = double(), hitrate = double(), falsealarmrate = double())
  
  # calculate hits and false alarms per participant
  
  for (i in 1:(length(levelindex)-1)) {
    
    hits <- length(which(data_nback$target[c(levelindex[i]:(levelindex[i+1])-1)] == 1 &
                           data_nback$correct[c(levelindex[i]:(levelindex[i+1])-1)] == 1))
    falsealarms <- length(which(data_nback$target[c(levelindex[i]:(levelindex[i+1])-1)] == 0 &
                           data_nback$correct[c(levelindex[i]:(levelindex[i+1])-1)] == 0))
    newdata <- data.frame(subject = data_nback$subject[levelindex[i]],
                          level = data_nback$level[levelindex[i]],
                          hitrate = hits/num_targets,
                          falsealarmrate = falsealarms/num_nontargets)
    dprime <- rbind(dprime, newdata)
    
  }
  
  # z-transform the hit rate and the false alarm rate
  
  for (i in 1:4) {
    
    mean_hitrate <- mean(dprime$hitrate[dprime$level == i])
    sd_hitrate <- sd(dprime$hitrate[dprime$level == i])
    mean_falsealarmrate <- mean(dprime$falsealarmrate[dprime$level == i])
    sd_falsealarmrate <- sd(dprime$falsealarmrate[dprime$level == i])
    
    for (j in 1:length(which(dprime$level == i))) {
      
      dprime$hitrate[which(dprime$level == i)[j]] <- (dprime$hitrate[which(dprime$level == i)[j]] - mean_hitrate)/sd_hitrate
      dprime$falsealarmrate[which(dprime$level == i)[j]] <- (dprime$falsealarmrate[which(dprime$level == i)[j]] -
                                                               mean_falsealarmrate)/sd_falsealarmrate
    }
  }
  
  # calculate d'
  
  dprime$d <- dprime$hitrate - dprime$falsealarmrate
  
  # ANOVA with three linear contrasts, contrasting the d’ of two n-back levels (2,3,4) at a time
  
  

```

